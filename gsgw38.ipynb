{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit1fe2bb4cf81446c4930ef0f70b991c44",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1 Import Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS ####\n",
    "import pandas as pd # for data manipulation and .describe()\n",
    "\n",
    "# #### READ IN DATA ####\n",
    "cov = pd.read_csv(\"datasets/latestdata.csv\") # original (can be switched out for smaller or outcome to run quicker)\n",
    "\n",
    "# Inspect data\n",
    "print(cov.count()) # print(cov[\"sex\"].describe() # also good\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 2 Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Drop irrelevant columns and rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT ONLY THE RELEVANT COLUMNS ####\n",
    "# Interesting problems which don't have quite enough data\n",
    "# fields = [\"age\", \"sex\", \"outcome\", \"date_admission_hospital\", \"date_onset_symptoms\", \"country\"] # 228 rows\n",
    "# fields = [\"outcome\", \"date_admission_hospital\", \"date_onset_symptoms\"] # 234 rows\n",
    "# fields = [\"outcome\", \"date_admission_hospital\", \"date_confirmation\"] # 262 rows\n",
    "# fields = [\"outcome\", \"date_confirmation\", \"date_onset_symptoms\"] # 3505 rows\n",
    "\n",
    "# Identify the columns required for the problem\n",
    "fields = [\"outcome\", \"age\", \"sex\", \"date_confirmation\", \"date_onset_symptoms\", \"country\"] # 3493 rows - mostly from the phillipines\n",
    "# Without the dates - worth having a look into\n",
    "# fields = [\"outcome\", \"country\", \"age\", \"sex\"] # 33599 rows\n",
    "\n",
    "# Select these columns from the dataset\n",
    "dataset = cov[fields]\n",
    "\n",
    "#### DATA CLEANING ####\n",
    " \n",
    "# Drop the rows which are missing information\n",
    "dataset = dataset.dropna(subset=fields)\n",
    "\n",
    "# Store the set for future use\n",
    "dataset.to_csv(\"datasets/dataset1.csv\")\n",
    "print('stored dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "read dataset from dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "# Instead of doing the above steps you can load the processed dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('datasets/dataset1.csv')\n",
    "print('read dataset from dataset1.csv')\n",
    "# Drop the unnamed column\n",
    "dataset = dataset.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "source": [
    "## 2.2 Feature encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of 0          79\n",
      "1          26\n",
      "2          25\n",
      "3          40\n",
      "4          43\n",
      "        ...  \n",
      "3488       44\n",
      "3489     61.0\n",
      "3490     32.0\n",
      "3491    50-59\n",
      "3492    20-29\n",
      "Name: age, Length: 3493, dtype: object>\n",
      "<bound method NDFrame.head of 0       0.797980\n",
      "1       0.262626\n",
      "2       0.252525\n",
      "3       0.404040\n",
      "4       0.434343\n",
      "          ...   \n",
      "3488    0.444444\n",
      "3489    0.616162\n",
      "3490    0.323232\n",
      "3491    0.555556\n",
      "3492    0.252525\n",
      "Name: age, Length: 3493, dtype: float64>\n",
      "/home/sam/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Tidy the outcome column\n",
    "dataset = dataset.replace(to_replace={'outcome': {\n",
    "    'died':0,\n",
    "    'death':0,\n",
    "    'Deceased':0,\n",
    "    'dead':0,\n",
    "    'stable':1,\n",
    "    'treated in an intensive care unit (14.02.2020)':1,\n",
    "    'Symptoms only improved with cough. Currently hospitalized for follow-up.':1, # TODO drop and compare results\n",
    "    'severe':0,        \n",
    "    'Hospitalized':1, # TODO drop and compare\n",
    "    'discharge':1,\n",
    "    'discharged':1,\n",
    "    'Discharged':1,\n",
    "    'Alive':1,\n",
    "    'recovered':1,\n",
    "    }})\n",
    "\n",
    "print(dataset['age'].head)\n",
    "\n",
    "# Tidy the ages column # TODO these need feature scaling\n",
    "dataset = dataset.replace(to_replace={'age': {\n",
    "    '0-9':5,\n",
    "    '10-19':15,\n",
    "    '20-29':25,\n",
    "    '30-39':35,\n",
    "    '40-49':45,\n",
    "    '50-59':55,\n",
    "    '60-69':65,\n",
    "    '70-79':75,\n",
    "    '80-89':85,\n",
    "    '90-99':95,\n",
    "    }}, regex=True)\n",
    "# age_df = pd.get_dummies(dataset['age'])\n",
    "\n",
    "# Apply feature scaling to the ages\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "dataset[['age']] = scaler.fit_transform(dataset[['age']])\n",
    "\n",
    "# dataset['age'] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(dataset['age'])),columns=['age'])\n",
    "\n",
    "print(dataset['age'].head)\n",
    "\n",
    "\n",
    "\n",
    "# Replace the two dates columns with days_waiting\n",
    "gaps = []\n",
    "from datetime import date, datetime, timedelta\n",
    "for i in range(len(dataset['date_confirmation'])):\n",
    "    dataset['date_confirmation'][i] = datetime.strptime(dataset['date_confirmation'][i], r'%d.%m.%Y')\n",
    "    dataset['date_onset_symptoms'][i] = datetime.strptime(dataset['date_onset_symptoms'][i], r'%d.%m.%Y')\n",
    "    gaps.append(dataset['date_confirmation'][i] - dataset['date_onset_symptoms'][i])\n",
    "\n",
    "dataset['days_waiting'] = gaps\n",
    "dataset = dataset.drop(columns=['date_confirmation', 'date_onset_symptoms'])\n",
    "dataset['days_waiting'] = dataset['days_waiting'].dt.days\n",
    "dataset = dataset[dataset['days_waiting'] >= 0]\n",
    "\n",
    "# Encode the sex data as integers\n",
    "dataset = dataset.replace(to_replace={'sex': {\n",
    "    'male':0,\n",
    "    'female':1\n",
    "}})\n",
    "\n",
    "# Encode the country data using one hot encoding\n",
    "countries_df = pd.get_dummies(dataset['country'])\n",
    "dataset = pd.concat([dataset, countries_df], axis=1)\n",
    "dataset = dataset.drop(columns=['country'])\n",
    "\n",
    "# Save the cleaned up dataset for future use\n",
    "dataset.to_csv('datasets/dataset1_clean.csv')"
   ]
  },
  {
   "source": [
    "## 2.3 Reweighting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4 Split the dataset into features/labels and test/train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of       outcome       age  sex  days_waiting  Angola  Brazil  Cabo Verde  \\\n0           1  0.323232    0            15       0       0           0   \n1           1  0.212121    1            20       0       0           0   \n2           1  0.202020    1             9       0       0           0   \n3           1  0.050505    1             5       0       0           0   \n4           1  0.323232    0             9       0       0           0   \n...       ...       ...  ...           ...     ...     ...         ...   \n3451        0  0.535354    1            21       0       0           0   \n3452        1  0.818182    1            25       0       0           0   \n3453        0  0.666667    1            28       0       0           0   \n3454        0  0.565657    0            23       0       0           0   \n3455        1  0.444444    1            18       0       0           0   \n\n      Canada  Central African Republic  China  ...  Gabon  Germany  India  \\\n0          0                         0      0  ...      0        0      0   \n1          0                         0      0  ...      0        0      0   \n2          0                         0      0  ...      0        0      0   \n3          0                         0      0  ...      0        0      0   \n4          0                         0      0  ...      0        0      0   \n...      ...                       ...    ...  ...    ...      ...    ...   \n3451       0                         0      0  ...      0        0      0   \n3452       0                         0      0  ...      0        0      0   \n3453       0                         0      0  ...      0        0      0   \n3454       0                         0      0  ...      0        0      0   \n3455       0                         0      0  ...      0        0      0   \n\n      Japan  Nepal  Philippines  Romania  Singapore  South Korea  Vietnam  \n0         0      0            1        0          0            0        0  \n1         0      0            1        0          0            0        0  \n2         0      0            1        0          0            0        0  \n3         0      0            1        0          0            0        0  \n4         0      0            1        0          0            0        0  \n...     ...    ...          ...      ...        ...          ...      ...  \n3451      0      0            1        0          0            0        0  \n3452      0      0            1        0          0            0        0  \n3453      0      0            1        0          0            0        0  \n3454      0      0            1        0          0            0        0  \n3455      0      0            1        0          0            0        0  \n\n[3456 rows x 21 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the rows of the dataframe so the order doesn't impact training\n",
    "# print(dataset.head)\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(dataset.head)\n",
    "\n",
    "# Split the data into features and labels\n",
    "features = dataset.drop(['outcome'], axis=1)\n",
    "labels = dataset['outcome']\n",
    "\n",
    "# print('features\\n', features.value_counts())\n",
    "# print('labels\\n', labels.value_counts())\n",
    "\n",
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "source": [
    "# 3 Visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4 Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:39:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "       use_label_encoder=True, validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "# Import and fit a Support Vector Machine\n",
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(gamma='auto', class_weight={0: 2832/624, 1:1})\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Immport and fit a logistic regression model\n",
    "from sklearn import linear_model\n",
    "# LR_classifier = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000) # MULTINOMIAL There are five solvers that can be used to obtain the weights \n",
    "LR_classifier = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000, class_weight={0: 2832/624, 1:1})\n",
    "LR_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a decision tree\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier(class_weight={0: 2832/624, 1:1})\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = GaussianNB()\n",
    "# nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # import and fit an XGboost model\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier()#use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "# 5 Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Use the models to make some predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset with each model\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "LR_predictions = LR_classifier.predict(X_test)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 5.2 Measure the accuracy of the predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM score: 0.6445086705202312\nLR score: 0.6705202312138728\nTree score: 0.7109826589595376\nNaive Bayes score: 0.23410404624277456\nXGB score: 0.7861271676300579\n"
     ]
    }
   ],
   "source": [
    "# Measure the accuracy of each set of predictions\n",
    "from sklearn import metrics\n",
    "svm_accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "LR_accuracy = metrics.accuracy_score(y_test, LR_predictions)\n",
    "tree_accuracy = metrics.accuracy_score(y_test, tree_predictions)\n",
    "# random forest\n",
    "nb_accuracy = metrics.accuracy_score(y_test, nb_predictions)\n",
    "xgb_accuracy = metrics.accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "print(f'SVM score: {svm_accuracy}\\nLR score: {LR_accuracy}\\nTree score: {tree_accuracy}\\nNaive Bayes score: {nb_accuracy}\\nXGB score: {xgb_accuracy}')\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 5.3 Produce a more detailed report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification report for svm (remember 0:died, 1:hospitalised, 2:recovered (0.805)): \n              precision    recall  f1-score   support\n\n           0       0.31      0.73      0.43        64\n           1       0.91      0.62      0.74       282\n\n   micro avg       0.64      0.64      0.64       346\n   macro avg       0.61      0.68      0.59       346\nweighted avg       0.80      0.64      0.68       346\n\n"
     ]
    }
   ],
   "source": [
    "# Produce classification reports\n",
    "\n",
    "print(f'Classification report for svm (remember 0:died, 1:hospitalised, 2:recovered (0.805)): \\n{metrics.classification_report(y_test, svm_predictions)}')\n",
    "# These scores are all around the 80% mark.\n",
    "# I'd like to know what the true and false positive rates are (precision = % of Positives that are correct and recall = % of negatives that are found)\n",
    "# I think it'll be necessary to adjust the sampling or reweight the inputs\n",
    "# get rid of hospitalised - they probably recovered\n",
    "# reweight the died class: for every entry repeat 4 times\n",
    "# make a few plots such as age/country and days waiting\n",
    "# Should I be doing k-fold validation? Do I have enough data for that?\n",
    "# There are also several model parameters that could be tuned"
   ]
  }
 ]
}