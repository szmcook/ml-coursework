{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit1fe2bb4cf81446c4930ef0f70b991c44",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1 Import Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS ####\n",
    "import pandas as pd # for data manipulation and .describe()\n",
    "\n",
    "# #### READ IN DATA ####\n",
    "cov = pd.read_csv(\"datasets/latestdata.csv\") # original (can be switched out for smaller or outcome to run quicker)\n",
    "\n",
    "# Inspect data\n",
    "print(cov.count()) # print(cov[\"sex\"].describe() # also good\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 2 Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Drop irrelevant columns and rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT ONLY THE RELEVANT COLUMNS ####\n",
    "# Interesting problems which don't have quite enough data\n",
    "# fields = [\"age\", \"sex\", \"outcome\", \"date_admission_hospital\", \"date_onset_symptoms\", \"country\"] # 228 rows\n",
    "# fields = [\"outcome\", \"date_admission_hospital\", \"date_onset_symptoms\"] # 234 rows\n",
    "# fields = [\"outcome\", \"date_admission_hospital\", \"date_confirmation\"] # 262 rows\n",
    "# fields = [\"outcome\", \"date_confirmation\", \"date_onset_symptoms\"] # 3505 rows\n",
    "\n",
    "# Identify the columns required for the problem\n",
    "fields = [\"outcome\", \"age\", \"sex\", \"date_confirmation\", \"date_onset_symptoms\", \"country\"] # 3493 rows - mostly from the phillipines\n",
    "# Without the dates - worth having a look into\n",
    "# fields = [\"outcome\", \"country\", \"age\", \"sex\"] # 33599 rows\n",
    "\n",
    "# Select these columns from the dataset\n",
    "dataset = cov[fields]\n",
    "\n",
    "#### DATA CLEANING ####\n",
    " \n",
    "# Drop the rows which are missing information\n",
    "dataset = dataset.dropna(subset=fields)\n",
    "\n",
    "# Store the set for future use\n",
    "dataset.to_csv(\"datasets/dataset1.csv\")\n",
    "print('stored dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of doing the above steps you can load the processed dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('datasets/dataset1.csv')\n",
    "print('read dataset from dataset1.csv')\n",
    "# Drop the unnamed column\n",
    "dataset = dataset.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "source": [
    "## 2.2 Feature encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy the outcome column\n",
    "dataset = dataset.replace(to_replace={'outcome': {\n",
    "    'died':0,\n",
    "    'death':0,\n",
    "    'Deceased':0,\n",
    "    'dead':0,\n",
    "    'stable':1,\n",
    "    'treated in an intensive care unit (14.02.2020)':1,\n",
    "    'Symptoms only improved with cough. Currently hospitalized for follow-up.':1, # TODO drop and compare results\n",
    "    'severe':0,        \n",
    "    'Hospitalized':1, # TODO drop and compare\n",
    "    'discharge':1,\n",
    "    'discharged':1,\n",
    "    'Discharged':1,\n",
    "    'Alive':1,\n",
    "    'recovered':1,\n",
    "    }})\n",
    "\n",
    "print(dataset['age'].head)\n",
    "\n",
    "# Tidy the ages column # TODO these need feature scaling\n",
    "dataset = dataset.replace(to_replace={'age': {\n",
    "    '0-9':5,\n",
    "    '10-19':15,\n",
    "    '20-29':25,\n",
    "    '30-39':35,\n",
    "    '40-49':45,\n",
    "    '50-59':55,\n",
    "    '60-69':65,\n",
    "    '70-79':75,\n",
    "    '80-89':85,\n",
    "    '90-99':95,\n",
    "    }}, regex=True)\n",
    "# age_df = pd.get_dummies(dataset['age'])\n",
    "\n",
    "# Apply feature scaling to the ages\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "dataset[['age']] = scaler.fit_transform(dataset[['age']])\n",
    "\n",
    "# dataset['age'] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(dataset['age'])),columns=['age'])\n",
    "\n",
    "print(dataset['age'].head)\n",
    "\n",
    "\n",
    "\n",
    "# Replace the two dates columns with days_waiting\n",
    "gaps = []\n",
    "from datetime import date, datetime, timedelta\n",
    "for i in range(len(dataset['date_confirmation'])):\n",
    "    dataset['date_confirmation'][i] = datetime.strptime(dataset['date_confirmation'][i], r'%d.%m.%Y')\n",
    "    dataset['date_onset_symptoms'][i] = datetime.strptime(dataset['date_onset_symptoms'][i], r'%d.%m.%Y')\n",
    "    gaps.append(dataset['date_confirmation'][i] - dataset['date_onset_symptoms'][i])\n",
    "\n",
    "dataset['days_waiting'] = gaps\n",
    "dataset = dataset.drop(columns=['date_confirmation', 'date_onset_symptoms'])\n",
    "dataset['days_waiting'] = dataset['days_waiting'].dt.days\n",
    "dataset = dataset[dataset['days_waiting'] >= 0]\n",
    "\n",
    "# Encode the sex data as integers\n",
    "dataset = dataset.replace(to_replace={'sex': {\n",
    "    'male':0,\n",
    "    'female':1\n",
    "}})\n",
    "\n",
    "# Encode the country data using one hot encoding\n",
    "countries_df = pd.get_dummies(dataset['country'])\n",
    "dataset = pd.concat([dataset, countries_df], axis=1)\n",
    "dataset = dataset.drop(columns=['country'])\n",
    "\n",
    "# Save the cleaned up dataset for future use\n",
    "dataset.to_csv('datasets/dataset1_clean.csv')"
   ]
  },
  {
   "source": [
    "## 2.3 Reweighting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4 Split the dataset into features/labels and test/train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "# features = dataset[['days_waiting', 'age', 'sex', 'country']]\n",
    "features = dataset.drop(['outcome'], axis=1)\n",
    "labels = dataset['outcome']\n",
    "\n",
    "print('features\\n', features.value_counts())\n",
    "print('labels\\n', labels.value_counts())\n",
    "\n",
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "source": [
    "# 3 Visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4 Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and fit a Support Vector Machine\n",
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(gamma='auto', class_weight={0: 2832/624, 1:1})\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Immport and fit a logistic regression model\n",
    "from sklearn import linear_model\n",
    "# LR_classifier = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000) # MULTINOMIAL There are five solvers that can be used to obtain the weights \n",
    "LR_classifier = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000, class_weight={0: 2832/624, 1:1})\n",
    "LR_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a decision tree\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier(class_weight={0: 2832/624, 1:1})\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = GaussianNB()\n",
    "# nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # import and fit an XGboost model\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier()#use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "# 5 Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Use the models to make some predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset with each model\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "LR_predictions = LR_classifier.predict(X_test)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 5.2 Measure the accuracy of the predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the accuracy of each set of predictions\n",
    "from sklearn import metrics\n",
    "svm_accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "LR_accuracy = metrics.accuracy_score(y_test, LR_predictions)\n",
    "tree_accuracy = metrics.accuracy_score(y_test, tree_predictions)\n",
    "# random forest\n",
    "nb_accuracy = metrics.accuracy_score(y_test, nb_predictions)\n",
    "xgb_accuracy = metrics.accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "print(f'SVM score: {svm_accuracy}\\nLR score: {LR_accuracy}\\nTree score: {tree_accuracy}\\nNaive Bayes score: {nb_accuracy}\\nXGB score: {xgb_accuracy}')\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 5.3 Produce a more detailed report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce classification reports\n",
    "\n",
    "print(f'Classification report for svm (remember 0:died, 1:hospitalised, 2:recovered (0.805)): \\n{metrics.classification_report(y_test, svm_predictions)}')\n",
    "# These scores are all around the 80% mark.\n",
    "# I'd like to know what the true and false positive rates are (precision = % of Positives that are correct and recall = % of negatives that are found)\n",
    "# I think it'll be necessary to adjust the sampling or reweight the inputs\n",
    "# get rid of hospitalised - they probably recovered\n",
    "# reweight the died class: for every entry repeat 4 times\n",
    "# make a few plots such as age/country and days waiting\n",
    "# Should I be doing k-fold validation? Do I have enough data for that?\n",
    "# There are also several model parameters that could be tuned"
   ]
  }
 ]
}