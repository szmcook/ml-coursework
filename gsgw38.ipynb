{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit1fe2bb4cf81446c4930ef0f70b991c44",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1 Import Dataset\n",
    "The first step is to import the dataset and have a look at which columns contain a lot of data points\n",
    "\n",
    "Please note that the program expects the dataset to be saved as latestdata.csv in the same folder as the program."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the dataset\n",
    "cov = pd.read_csv(\"latestdata.csv\")\n",
    "print('Read dataset from latestdata.csv')\n",
    "\n",
    "# Inspect data\n",
    "print(cov.count())"
   ]
  },
  {
   "source": [
    "# 2 Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Drop irrelevant columns and rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns required for the problem\n",
    "fields = [\"outcome\", \"age\", \"sex\", \"date_confirmation\", \"date_onset_symptoms\", \"country\"] # 3493 rows - mostly from the phillipines\n",
    "dataset = cov[fields]\n",
    " \n",
    "# Drop the rows which are missing information\n",
    "dataset = dataset.dropna(subset=fields)\n",
    "\n",
    "# Store the set for future use\n",
    "dataset.to_csv(\"dataset.csv\")\n",
    "print('Stored dataset as dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of doing the above steps you can load the processed dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "print('Read dataset from dataset.csv')\n",
    "# Drop the unnamed column\n",
    "dataset = dataset.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(dataset.count())"
   ]
  },
  {
   "source": [
    "## 2.2 Feature encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy the outcome column\n",
    "dataset = dataset.replace(to_replace={'outcome': {\n",
    "    'died':0,\n",
    "    'death':0,\n",
    "    'Deceased':0,\n",
    "    'dead':0,\n",
    "    'stable':1,\n",
    "    'treated in an intensive care unit (14.02.2020)':1,\n",
    "    'Symptoms only improved with cough. Currently hospitalized for follow-up.':1, # TODO drop and compare results\n",
    "    'severe':0,        \n",
    "    'Hospitalized':1, # TODO drop and compare results\n",
    "    'discharge':1,\n",
    "    'discharged':1,\n",
    "    'Discharged':1,\n",
    "    'Alive':1,\n",
    "    'recovered':1,\n",
    "    }})\n",
    "\n",
    "# Tidy the ages column\n",
    "dataset = dataset.replace(to_replace={'age': {\n",
    "    '0-9':5,\n",
    "    '10-19':15,\n",
    "    '20-29':25,\n",
    "    '30-39':35,\n",
    "    '40-49':45,\n",
    "    '50-59':55,\n",
    "    '60-69':65,\n",
    "    '70-79':75,\n",
    "    '80-89':85,\n",
    "    '90-99':95,\n",
    "    }}, regex=True)\n",
    "\n",
    "# Apply feature scaling to the ages\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "dataset[['age']] = scaler.fit_transform(dataset[['age']])\n",
    "\n",
    "# Replace the two dates columns with days_waiting\n",
    "gaps = []\n",
    "from datetime import date, datetime, timedelta\n",
    "for i in range(len(dataset['date_confirmation'])):\n",
    "    dataset['date_confirmation'][i] = datetime.strptime(dataset['date_confirmation'][i], r'%d.%m.%Y')\n",
    "    dataset['date_onset_symptoms'][i] = datetime.strptime(dataset['date_onset_symptoms'][i], r'%d.%m.%Y')\n",
    "    gaps.append(dataset['date_confirmation'][i] - dataset['date_onset_symptoms'][i])\n",
    "\n",
    "dataset['days_waiting'] = gaps\n",
    "dataset = dataset.drop(columns=['date_confirmation', 'date_onset_symptoms'])\n",
    "dataset['days_waiting'] = dataset['days_waiting'].dt.days\n",
    "dataset = dataset[dataset['days_waiting'] >= 0]\n",
    "\n",
    "# Encode the sex data as integers\n",
    "dataset = dataset.replace(to_replace={'sex': {\n",
    "    'male':0,\n",
    "    'female':1\n",
    "}})\n",
    "\n",
    "# Encode the country data using one hot encoding\n",
    "countries_df = pd.get_dummies(dataset['country'])\n",
    "dataset = pd.concat([dataset, countries_df], axis=1)\n",
    "dataset = dataset.drop(columns=['country'])\n",
    "\n",
    "# Save the cleaned up dataset for future use\n",
    "dataset.to_csv('dataset_clean.csv')\n",
    "print('Saved dataset as dataset_clean.csv')"
   ]
  },
  {
   "source": [
    "# 3 Visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "data = dataset['age']\n",
    "sns.histplot(data=data, ax=ax[0])\n",
    "\n",
    "data = dataset.loc[dataset['outcome'] == 0, 'age']\n",
    "sns.histplot(data=data, ax=ax[1]).set_title('Died')\n",
    "data = dataset.loc[dataset['outcome'] == 1, 'age']\n",
    "sns.histplot(data=data, ax=ax[2]).set_title('Recovered')\n",
    "\n",
    "print('Produced graphs to show the distribution of the scaled ages for the dataset, those that died and those that recovered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "data = dataset['sex']\n",
    "sns.histplot(data=data, ax=ax[0])\n",
    "\n",
    "data = dataset.loc[dataset['outcome'] == 0, 'sex']\n",
    "sns.histplot(data=data, ax=ax[1]).set_title('Died')\n",
    "data = dataset.loc[dataset['outcome'] == 1, 'sex']\n",
    "sns.histplot(data=data, ax=ax[2]).set_title('Recovered')\n",
    "\n",
    "print('Produced graphs to show the distribution of the genders for the dataset, those that died and those that recovered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "data = dataset['days_waiting']\n",
    "sns.histplot(data=data, ax=ax[0])\n",
    "\n",
    "data = dataset.loc[dataset['outcome'] == 0, 'days_waiting']\n",
    "sns.histplot(data=data, ax=ax[1]).set_title('Died')\n",
    "data = dataset.loc[dataset['outcome'] == 1, 'days_waiting']\n",
    "sns.histplot(data=data, ax=ax[2]).set_title('Recovered')\n",
    "\n",
    "print('produced graphs to show the distribution of the number of days between the onset of symptoms and the date of confirmation for the dataset, those that died and those that recovered')"
   ]
  },
  {
   "source": [
    "# 4 Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.1 Split the dataset into features/labels and test/train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "features = dataset.drop(['outcome'], axis=1)\n",
    "labels = dataset['outcome']\n",
    "\n",
    "# print('features\\n', features.value_counts())\n",
    "# print('labels\\n', labels.value_counts())\n",
    "\n",
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0) # This also shuffles the data"
   ]
  },
  {
   "source": [
    "## 4.2 Fit the models\n",
    "In this section we fit 6 different models for comparison. From this 6 I will choose the best performing 3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and fit a Support Vector Machine\n",
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(gamma='auto', class_weight={0: 2832/624, 1:1})\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Immport and fit a logistic regression model\n",
    "from sklearn import linear_model\n",
    "# LR_classifier = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000) # MULTINOMIAL There are five solvers that can be used to obtain the weights \n",
    "LR_classifier = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000, class_weight={0: 2832/624, 1:1})\n",
    "LR_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a decision tree\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier(class_weight={0: 2832/624, 1:1})\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import a random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_classifier = RandomForestClassifier(class_weight={0: 2832/624, 1:1}, n_estimators=100)\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Import and fit a naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# nb_classifier = GaussianNB(priors=[624/3456, 2832/3456])\n",
    "nb_classifier = GaussianNB(priors=[2832/3456, 624/3456])\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # import and fit an XGboost model\n",
    "import xgboost.sklearn as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(class_weight={0: 2832/624, 1:1}, use_label_encoder=False, verbosity=0)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "print('Trained a selection of models on the training data')"
   ]
  },
  {
   "source": [
    "# 5 Model Evaluation\n",
    "\n",
    "In this section we look at how the models performed on the test set. \n",
    "This allows us to select the top 3 models before attempting to improve their performance through hyperparameter tuning.\n",
    "\n",
    "The evaluation metrics of interest at this step are the accuracy, balanced_accuracy and f1_score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Use the models to make some predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset with each model\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "LR_predictions = LR_classifier.predict(X_test)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "forest_predictions = forest_classifier.predict(X_test)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n"
   ]
  },
  {
   "source": [
    "## 5.2 Measure the accuracy of the predictions\n",
    "\n",
    "The accuracy is immediately easy to interpret and is therefore worth examining before looking at other metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the accuracy of each set of predictions\n",
    "from sklearn import metrics\n",
    "svm_accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "LR_accuracy = metrics.accuracy_score(y_test, LR_predictions)\n",
    "tree_accuracy = metrics.accuracy_score(y_test, tree_predictions)\n",
    "forest_accuracy = metrics.accuracy_score(y_test, forest_predictions)\n",
    "nb_accuracy = metrics.accuracy_score(y_test, nb_predictions)\n",
    "xgb_accuracy = metrics.accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "print(f'SVM score: {svm_accuracy}\\nLR score: {LR_accuracy}\\nTree score: {tree_accuracy}\\nForest score: {forest_accuracy}\\nNaive Bayes score: {nb_accuracy}\\nXGB score: {xgb_accuracy}')\n"
   ]
  },
  {
   "source": [
    "## 5.3 View more detailed classification reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce classification reports\n",
    "\n",
    "# precision = % of Positives that are correct and recall = % of negatives that are found)\n",
    "# The recall for a class is the sensitivity to that class (if it exists is it identified)\n",
    "# The precision for a class is the \n",
    "\n",
    "def evaluate(model_name, predictions):\n",
    "    print('\\033[95m' + model_name.upper() + '\\033[0m')\n",
    "    scores = pd.DataFrame()\n",
    "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, predictions)\n",
    "    f1score = metrics.f1_score(y_test, predictions)\n",
    "    scores[model_name] = [accuracy, balanced_accuracy, f1score]\n",
    "    scores.index = ['Accuracy', 'Balanced Accuracy', 'f1 Score']\n",
    "    print(scores)\n",
    "    print(f'\\nConfusion matrix for {model_name}:')\n",
    "    print(metrics.confusion_matrix(y_test, predictions))\n",
    "    print(f'\\nFull classification report for {model_name}:')\n",
    "    print(metrics.classification_report(y_test, predictions, target_names=['died','recovered']))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "evaluate('SVM', svm_predictions)\n",
    "evaluate('Logistic Regression', LR_predictions)\n",
    "evaluate('Decision tree', tree_predictions)\n",
    "evaluate('Random forest', forest_predictions)\n",
    "evaluate('Naive Bayes', nb_predictions)\n",
    "evaluate('XGBoost', xgb_predictions)"
   ]
  },
  {
   "source": [
    "At this point we can see that the f1 score.....?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6 Model Improvement\n",
    "\n",
    "This section involves trying a cross validation method and hyperparameter tuning to try and improve the models\n",
    "The tuning actually occurs during the .fit() call on the GridSearchCV object.\n",
    "Calling this on the training data means cross validation folds are automatically generated from the training data and the test data remains unseen (to be used for evaluation later).\n",
    "Because of this, we can see that the accuracy of the models is lower than the previous accuracy but we hope that the model quality has improved regardless. This will be tested at the end.\n",
    "\n",
    "For each model, we want to find the best set of parameters to improve the ____.\n",
    "\n",
    "We want to choose from:\n",
    "- 'accuracy'\n",
    "- 'balanced_accuracy'\n",
    "- 'f1'\n",
    "- 'f1_weighted'\n",
    "- 'precision'\n",
    "- 'recall'\n",
    "- 'roc_auc'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "source": [
    "## 6.1 Tune the random forest hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "class_weight = [{0: 2832/624, 1:1}]\n",
    "\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "criterions = ['gini', 'entropy']\n",
    "max_depths = [None, 10, 100]\n",
    "max_featuress = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "rf_param_grid = dict(class_weight=class_weight, n_estimators=n_estimators, criterion=criterions, max_depth=max_depths, max_features=max_featuress)\n",
    "\n",
    "# svm_grid_search = GridSearchCV(estimator=model, param_grid=svm_param_grid, n_jobs=-1, cv=5, scoring='accuracy', error_score=0, verbose=1)\n",
    "rf_grid_search = RandomizedSearchCV(estimator=model, param_distributions=rf_param_grid, n_jobs=-1, cv=5, scoring='balanced_accuracy', error_score=0, verbose=1)\n",
    "rf_grid_result = rf_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best: {rf_grid_result.best_score_} using {rf_grid_result.best_params_}\")"
   ]
  },
  {
   "source": [
    "## 6.1 Tune the SVM hyperparameters\n",
    "We will tune the kernel (and where relevant the gamma value) as well as the C values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune SVM hyperparameters\n",
    "model = svm.SVC()\n",
    "# print(model.get_params())\n",
    "class_weight = [{0: 2832/624, 1:1}]\n",
    "kernels = ['rbf', 'linear']\n",
    "gammas = ['auto', 'scale']\n",
    "c_values = [100, 10, 1, 0.1, 0.01]\n",
    "\n",
    "svm_param_grid = dict(class_weight=class_weight, kernel=kernels, C=c_values, gamma=gammas)\n",
    "# svm_grid_search = GridSearchCV(estimator=model, param_grid=svm_param_grid, n_jobs=-1, cv=5, scoring='accuracy', error_score=0, verbose=1)\n",
    "svm_grid_search = RandomizedSearchCV(estimator=model, param_distributions=svm_param_grid, n_jobs=-1, cv=5, scoring='balanced_accuracy', error_score=0, verbose=1)\n",
    "svm_grid_result = svm_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best: {svm_grid_result.best_score_} using {svm_grid_result.best_params_}\")"
   ]
  },
  {
   "source": [
    "## 6.2 Tune the hyperparameters for Logistic Regression\n",
    "We will tune the solvers, C value and max_iterations parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune logistic regression hyperparameters\n",
    "model = linear_model.LogisticRegression()\n",
    "# print(model.get_params())\n",
    "class_weight = [2832/624, 1]\n",
    "solvers = ['lbfgs', 'liblinear']\n",
    "c_values = [100, 10, 1, 0.1, 0.01]\n",
    "max_iter = [500, 1000]\n",
    "lr_param_grid = dict(class_weight=class_weight, solver=solvers, C=c_values, max_iter=max_iter)\n",
    "# lr_grid_search = GridSearchCV(estimator=model, param_grid=lr_param_grid, n_jobs=-1, cv=5, scoring='accuracy', error_score=0, verbose=1)\n",
    "lr_grid_search = RandomizedSearchCV(estimator=model, param_distributions=lr_param_grid, n_jobs=-1, cv=5, scoring='accuracy', error_score=0, verbose=1)\n",
    "lr_grid_result = lr_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best: {lr_grid_result.best_score_} using {lr_grid_result.best_params_}\")"
   ]
  },
  {
   "source": [
    "## 6.3 Tune the XGBoost parameters\n",
    "We tune the XGBoost hyperparameters in sections to cut running time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune SVM hyperparameters\n",
    "model = xgb.XGBClassifier()\n",
    "# print(model.get_params())\n",
    "\n",
    "class_weight = [{0: 2832/624, 1:1}]\n",
    "verbosity=[0]\n",
    "\n",
    "max_depths = list(range(3,10)) # best was 3; default=6\n",
    "min_child_weights = list(range(0,4)) # best was 3; default=1\n",
    "\n",
    "gammas = [i/10.0 for i in range(0,5)] # default 0, best=0.2\n",
    "\n",
    "subsamples = [i/10.0 for i in range(6,14)] # default=1, best 0.6\n",
    "colsample_bytrees = [i/10.0 for i in range(6,14)] # default=1, best 0.9\n",
    "\n",
    "reg_alphas = [0, 1e-5, 1e-2, 0.1, 1, 100] # default=0, best=100\n",
    "\n",
    "xgb_param_grid = dict(class_weight=class_weight, max_depth=max_depths, min_child_weight=min_child_weights, gamma=gammas, subsample=subsamples, colsample_bytree=colsample_bytrees, reg_alpha=reg_alphas, verbosity=verbosity)\n",
    "\n",
    "# xgb_grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_grid, n_jobs=-1, cv=5, scoring='accuracy', error_score=0, verbose=1)\n",
    "xgb_grid_search = RandomizedSearchCV(estimator=model, param_distributions=xgb_param_grid, n_jobs=-1, cv=5, scoring='balanced_accuracy', error_score=0, verbose=1)\n",
    "xgb_grid_result = xgb_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best: {xgb_grid_result.best_score_} using {xgb_grid_result.best_params_}\")"
   ]
  },
  {
   "source": [
    "# Final models\n",
    "\n",
    "Using the best parameters determined by the hyper parameter tuning we can create a final set of classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(\n",
    "    class_weight={0: 2832/624, 1:1},\n",
    "    kernel='linear',\n",
    "    gamma='scale',\n",
    "    C=0.1\n",
    ")\n",
    "# 0.7567 on the training data\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    class_weight={0: 2832/624, 1:1},\n",
    "    n_estimators=200,\n",
    "    max_features='sqrt',\n",
    "    max_depth=10,\n",
    "    criterion='gini'\n",
    ")\n",
    "# 0.7424 on the training data\n",
    "\n",
    "LR_classifier = linear_model.LogisticRegression(\n",
    "    class_weight={0: 2832/624, 1:1},\n",
    "    solver='lbfgs', \n",
    "    max_iter=500,\n",
    "    C=1\n",
    ")\n",
    "# 0.8315 on the training data\n",
    "\n",
    "# xgb.config_context(verbosity=0)\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    class_weight={0: 2832/624, 1:1},\n",
    "    use_label_encoder=False,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=1e-05,\n",
    "    min_child_weight=0,\n",
    "    max_depth=8,\n",
    "    gamma=0.4,\n",
    "    colsample_bytree=0.6,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "LR_classifier.fit(X_train, y_train)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "print('Models trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "forest_predictions = rf_classifier.predict(X_test)\n",
    "LR_predictions = LR_classifier.predict(X_test)\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "evaluate('SVM', svm_predictions)\n",
    "evaluate('Logistic Regression', LR_predictions)\n",
    "evaluate('Random forest', forest_predictions)\n",
    "evaluate('XGBoost', xgb_predictions)"
   ]
  },
  {
   "source": [
    "# Workflow steps\n",
    "- [x] Problem framing (define a problem, find a dataset and define a measure of success)\n",
    "- [x] Data preparation (identify the features and split into training and testing)\n",
    "- [x] Model selection (try several models and see what you get)\n",
    "- [x] Model training\n",
    "- [x] Model testing (evaluate the models against the measure of success)\n",
    "- [x] Hyperparameter tuning (attempt to further improve the model by tuning the hyperparameters)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}